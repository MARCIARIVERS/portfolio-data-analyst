# ğŸ§ª AnÃ¡lisis A/B â€“ EvaluaciÃ³n de Comportamiento de Usuarios

## ğŸ“Œ Objetivo del proyecto
Evaluar el impacto de un experimento A/B sobre el comportamiento de los usuarios, comparando mÃ©tricas clave entre grupos de control y prueba para determinar si existen diferencias estadÃ­sticamente significativas.

---

## ğŸ› ï¸ Herramientas utilizadas
- Python  
- Pandas  
- NumPy  
- SciPy  
- Matplotlib  

## ğŸ“‚ DescripciÃ³n del dataset
El conjunto de datos contiene eventos generados por usuarios dentro de una aplicaciÃ³n, incluyendo:

- `EventName`: nombre del evento  
- `DeviceIDHash`: identificador Ãºnico del usuario  
- `EventTimestamp`: fecha y hora del evento  
- `ExpId`: identificador del experimento  

### Grupos del experimento:
- **Control:** 246 y 247  
- **Test:** 248

## ğŸ§¹ Limpieza de datos
Se realizaron los siguientes pasos:
- ConversiÃ³n de fechas a formato datetime  
- VerificaciÃ³n de valores nulos  
- EliminaciÃ³n de duplicados  
- ValidaciÃ³n de distribuciÃ³n de usuarios por grupo

## ğŸ“Š AnÃ¡lisis realizado

### ğŸ”¹ DistribuciÃ³n de usuarios por grupo
Se validÃ³ que los grupos tuvieran tamaÃ±os comparables para asegurar un experimento equilibrado.

### ğŸ”¹ AnÃ¡lisis de eventos
Se analizÃ³ el nÃºmero de eventos por usuario para evaluar el nivel de interacciÃ³n.

### ğŸ”¹ ComparaciÃ³n estadÃ­stica
Se utilizÃ³ la **prueba de Mann-Whitney U** para comparar el comportamiento entre grupos de control y prueba.

## ğŸ“‰ Resultados
- No se encontraron diferencias estadÃ­sticamente significativas entre los grupos.
- El comportamiento de los usuarios fue consistente entre control y prueba.
- El cambio evaluado no tuvo impacto medible en la interacciÃ³n.

## âœ… Conclusiones
- El experimento no mostrÃ³ mejoras significativas.
- Se recomienda no implementar el cambio evaluado.
- Es necesario considerar nuevas hipÃ³tesis o mÃ©tricas para futuras pruebas A/B.




















